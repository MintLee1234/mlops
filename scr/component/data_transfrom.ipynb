{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from datetime import datetime\n",
    "\n",
    "from category_encoders import TargetEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, df):\n",
    "        self.df = self.transform_data(df)\n",
    "    def transform_data(self, df):\n",
    "        \"\"\"\n",
    "        Performs data transformations on the input DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert data to appropriate dtypes\n",
    "        numerical_columns = ['age', 'days_since_last_login', 'avg_time_spent',\n",
    "                            'avg_transaction_value', 'avg_frequency_login_days', 'points_in_wallet', 'churn_risk_score']\n",
    "        df[numerical_columns] = df[numerical_columns].apply(pd.to_numeric, errors='coerce')\n",
    "        df['last_visit_time'] = pd.to_datetime(df['last_visit_time'], format='%H:%M:%S')\n",
    "        categorical_columns = ['gender', 'region_category', 'membership_category',\n",
    "                            'joined_through_referral', 'preferred_offer_types', 'medium_of_operation',\n",
    "                            'internet_option', 'used_special_discount', 'offer_application_preference',\n",
    "                            'past_complaint', 'complaint_status', 'feedback']\n",
    "        df[categorical_columns] = df[categorical_columns].astype('object')\n",
    "        df['joining_date'] = pd.to_datetime(df['joining_date'])\n",
    "\n",
    "        # Impute missing values\n",
    "        # Iterative Imputer for numerical columns\n",
    "        target_column = 'churn_risk_score'\n",
    "        numeric_columns = df.select_dtypes(include='number').columns.drop(target_column)\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = df.copy()\n",
    "        df_scaled[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "        iterative_imputer = SimpleImputer(random_state=42)\n",
    "        df_scaled[numeric_columns] = iterative_imputer.fit_transform(df_scaled[numeric_columns])\n",
    "        df[numeric_columns] = scaler.inverse_transform(df_scaled[numeric_columns])\n",
    "\n",
    "        # KNN Imputer for categorical columns\n",
    "        df['gender'] = df['gender'].replace('Unknown', np.nan)\n",
    "        categorical_columns = ['gender', 'region_category', 'joined_through_referral', 'medium_of_operation',\n",
    "                            'preferred_offer_types']\n",
    "        encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        encoder.fit(df[categorical_columns])\n",
    "        df[categorical_columns] = encoder.transform(df[categorical_columns])\n",
    "        imputer = KNNImputer(n_neighbors=5, metric='nan_euclidean', weights='distance')\n",
    "        df[categorical_columns] = imputer.fit_transform(df[categorical_columns])\n",
    "        for col in categorical_columns:\n",
    "            df[col] = df[col].astype('object')\n",
    "\n",
    "        # Feature Engineering\n",
    "        specific_date = datetime(2024, 5, 17)\n",
    "        df['tenure_months'] = ((specific_date.year - df['joining_date'].dt.year) * 12 +\n",
    "                            (specific_date.month - df['joining_date'].dt.month)).astype('int64')\n",
    "        df['visit_hour'] = df['last_visit_time'].dt.hour.astype('int64')\n",
    "        df['login_spend_ratio'] = df['avg_time_spent'] / df['avg_frequency_login_days']\n",
    "        df['login_transaction_ratio'] = df['avg_frequency_login_days'] / df['avg_transaction_value']\n",
    "\n",
    "        # Target column class distribution\n",
    "        mapping = {\n",
    "            -1: 0,\n",
    "            0: 0,\n",
    "            2: 0,\n",
    "            3: 1,\n",
    "            4: 2,\n",
    "            5: 2\n",
    "        }\n",
    "\n",
    "        df['churn_risk_score'] = df['churn_risk_score'].map(mapping)\n",
    "\n",
    "        df = df.drop(columns=['joining_date', 'last_visit_time'])\n",
    "\n",
    "        # Rename columns\n",
    "        rename_mapping = {\n",
    "            'avg_frequency_login_days': 'frequency',\n",
    "            'avg_transaction_value': 'monetary',\n",
    "            'days_since_last_login': 'recency'\n",
    "        }\n",
    "\n",
    "        df = df.rename(columns=rename_mapping)\n",
    "\n",
    "        return df\n",
    "    def get_transformer_obj(self, X_train, y_train):\n",
    "            numerical_cols = self.config.numerical_cols\n",
    "            categorical_cols = self.config.categorical_cols\n",
    "\n",
    "            numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', MinMaxScaler())\n",
    "            ])\n",
    "\n",
    "            categorical_transformer = Pipeline(steps=[\n",
    "                ('target_encoder', TargetEncoder(cols=categorical_cols))\n",
    "\n",
    "            ])\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('numerical', numeric_transformer, numerical_cols),\n",
    "                    ('categorical', categorical_transformer, categorical_cols)\n",
    "                ],\n",
    "                remainder='passthrough'\n",
    "            )\n",
    "\n",
    "            preprocessor.fit(X_train, y_train)\n",
    "\n",
    "            return preprocessor\n",
    "\n",
    "\n",
    "    def train_val_test_splitting(self):\n",
    "            X = self.df.drop(columns=[\"churn_risk_score\"])\n",
    "            y = self.df[\"churn_risk_score\"]\n",
    "            X_train, X_rem, y_train, y_rem = train_test_split(X, y, test_size=0.3, random_state=42)  # 70% train, 30% remaining\n",
    "            X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=42) # Split remaining 30% \n",
    "\n",
    "\n",
    "            return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    def initiate_data_transformation(self, X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "\n",
    "            preprocessor_obj = self.get_transformer_obj(X_train, y_train)\n",
    "\n",
    "            X_train_transformed = preprocessor_obj.transform(X_train)\n",
    "            X_val_transformed = preprocessor_obj.transform(X_val)\n",
    "            X_test_transformed = preprocessor_obj.transform(X_test)\n",
    "\n",
    "            return X_train_transformed, X_val_transformed, X_test_transformed, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_translate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
